{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Crawler to download all the books from All It Ebooks site. \n",
    "It fetches all the ebook links from the sitemap. A new directory \n",
    "will be created and then all the files will be downloaded there, \n",
    "and then saves them in a JSON file. \n",
    "'''\n",
    "\n",
    "from lxml import etree, html\n",
    "from requests.adapters import HTTPAdapter\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "aite_sm = 'http://www.allitebooks.com/post-sitemap{0}.xml'\n",
    "\n",
    "urls = []\n",
    "\n",
    "with open('list-of-post-urls-allitbooks.md', 'w') as fp:\n",
    "    for i in range(1, 8):\n",
    "        fp.write('\\n\\n## Post Sitemap {0}\\n\\n'.format(i))\n",
    "        req = requests.get(aite_sm.format(i))\n",
    "        root = etree.fromstring(req.content)\n",
    "        for sitemap in root:\n",
    "            url = sitemap.getchildren()[0].text\n",
    "            fp.write('\\n[{0}]({1})\\n'.format(url, url))\n",
    "            urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "down_dict = {}\n",
    "\n",
    "if not os.path.exists('all-it-ebooks'):\n",
    "    os.mkdir('all-it-ebooks')\n",
    "\n",
    "save_dir = os.path.abspath(os.path.join(os.curdir, 'all-it-ebooks'))\n",
    "\n",
    "print('Crawling in Progress ......\\n')\n",
    "\n",
    "for i, url in enumerate(urls[1:]):    \n",
    "    page = requests.Session()\n",
    "    page.mount(url, HTTPAdapter(max_retries=5))\n",
    "    page = page.get(url)\n",
    "    tree = html.fromstring(page.content)    \n",
    "    down_link = tree.xpath(\"//*[@class=\\\"download-links\\\"]/a/@href\")\n",
    "    file_name = down_link[0].split('/')[-1]\n",
    "    title = re.sub('[^A-Za-z0-9]+', '-', file_name.split('.')[0])        \n",
    "    down_dict[title] = down_link[0]    \n",
    "    save_loc = os.path.join(save_dir, file_name)\n",
    "    \n",
    "    #Starting to download the files\n",
    "    data = requests.get(down_link[0], stream=True)\n",
    "    if not os.path.exists(save_loc):\n",
    "        print('\\nNow writing {0} - {1}'.format(i + 1, file_name))\n",
    "        with open(save_loc, 'wb') as f:\n",
    "            print(url)            \n",
    "            for chunk in data.iter_content(chunk_size=4096):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "    else:\n",
    "        print('\\nFile Exists. Skipped {0} - {1}'.format(i + 1, file_name))\n",
    "        \n",
    "print('All Urls have been crawled and saved.')\n",
    "print('\\nWriting links to JSON file.\\n')\n",
    "\n",
    "with open('all-it-ebooks-download-links.json', 'w') as fp:\n",
    "    json.dump(down_dict, fp, indent=4)\n",
    "    print('\\nWriting Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
