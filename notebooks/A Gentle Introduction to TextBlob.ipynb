{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to TextBlob\n",
    "___\n",
    "\n",
    "**TextBlob** is a Python (2 and 3) library for processing textual data. It provides a consistent API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, and more. **TextBlob objects can be treated as if they were Python strings that learned how to do Natural Language Processing.** TextBlob heavily depends on Python NLTK and pattern module by CLIPS. Corpora used by NLTK is the default corpora for TextBlob as well. For Installation insturctions [Click Here](https://textblob.readthedocs.org/en/dev/install.html)\n",
    "\n",
    "Today we will have an overview of this library in this part and our main focus will be to cover the different properties and methods of [BaseBlob](https://textblob.readthedocs.org/en/dev/api_reference.html#module-textblob.blob) class.\n",
    "\n",
    "## Basic's of TextBlob and Tokenization\n",
    "___\n",
    "\n",
    "First we import the TextBlob class which can be said as the most important class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We import the most important class TextBlob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will analyse and apply all the methods and functions of TextBlob on the following paragraph and sometimes with some additional sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = \"\"\"\n",
    "Hello, My name is Animesh Shaw and I am an undergraduate and studying Computer Science (upcoming Graduation in 2015). I Love programming and Computer science subjects of topics. \n",
    "My field of interest include Computational Linguistics. I Love watching anime specially One Piece and Naruto Shippunden. Animes specially those two shows\n",
    "an extravagant amount of dedication, passion love, and amibition towards achieving one's goal and aim's in life. These have always inpired me a lot.\n",
    "Giving up on your own dreams to fulfill others and the same feeling that other carry along with friends or something which I call as an eternal bond.\n",
    "I recommend everyone to watch Naruto and One Piece. I have learnt a lot from there. \"People are not always born intelligent or powerfull but with hard work\n",
    "great heights can be achieved in life.\" Yes its true, dedication and hard work are the key goals to success less than 1% people are born with the blessing\n",
    "of being a prodigy. The world was shaken mostly by those non-prodigy people which have had a massive impact in every individuals lives. \n",
    "With great goals and constant dedication and passion you can achieve \n",
    "the unachievable.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To use any functions or methods of TextBlob we first create a TextBlob object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tblob = TextBlob(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will store all the words of the paragraph along with their POS tags in a variable *tags*. tags is a property in TextBlob class which returns a list of tuples. The tuple format being (<word>, <POS>). All strings or return values in TextBlob are unicode encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tags = tblob.tags #We have stored the words of the text with the respective parts of speech tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Hello', u'UH'),\n",
       " (u'My', u'PRP$'),\n",
       " (u'name', u'NN'),\n",
       " (u'is', u'VBZ'),\n",
       " (u'Animesh', u'NNP'),\n",
       " (u'Shaw', u'NNP')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags[:6] # Now that the tags are stored we will display the first 6 tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNP stands for proper noun. It is used for name, place, animals etc. etc. PRP stands for Pronoun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets prints all the tags which was earlier stored in tags variable. We will just print the first 20 tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UH \n",
      "PRP$ \n",
      "NN \n",
      "VBZ \n",
      "NNP \n",
      "NNP \n",
      "CC \n",
      "PRP \n",
      "VBP \n",
      "DT \n",
      "JJ \n",
      "CC \n",
      "VBG \n",
      "NNP \n",
      "NNP \n",
      "JJ \n",
      "NNP \n",
      "IN \n",
      "CD \n",
      "PRP \n"
     ]
    }
   ],
   "source": [
    "for tag in tags[:20]:\n",
    "    print(str(tag[1]) + \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the above by writing a single line of code too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UH\n",
      "PRP$\n",
      "NN\n",
      "VBZ\n",
      "NNP\n",
      "NNP\n",
      "CC\n",
      "PRP\n",
      "VBP\n",
      "DT\n",
      "JJ\n",
      "CC\n",
      "VBG\n",
      "NNP\n",
      "NNP\n",
      "JJ\n",
      "NNP\n",
      "IN\n",
      "CD\n",
      "PRP\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join([tag[1] for tag in tags[:20]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us have a look at all the tags in the data above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UH PRP$ NN VBZ NNP NNP CC PRP VBP DT JJ CC VBG NNP NNP JJ NNP IN CD PRP NNP NN CC NNP NN NNS IN NNS PRP$ NN IN NN VBP NNP NNP PRP NNP VBG NN RB CD NNP CC NNP NNP NNP RB DT CD VBZ DT JJ NN IN NN NN NN CC NN IN VBG CD POS PRP NN CC NN POS PRP IN NN DT VBP RB VBN PRP DT NN VBG IN IN PRP$ JJ NNS TO VB NNS CC DT JJ NN IN JJ VB IN IN NNS CC NN WDT PRP VB IN DT JJ NN PRP VB NN TO VB NNP CC CD NNP PRP VBP NN DT NN IN EX NNS VBP RB RB VBN JJ CC NN CC IN JJ NN JJ NNS MD VB VBN IN NN UH PRP$ JJ NN CC JJ NN VBP DT JJ NNS TO NN JJR IN CD NNS VBP VBN IN DT NN IN VBG DT NN DT NN VBD VBN RB IN DT JJ NNS WDT VBP VBD DT JJ NN IN DT NNS NNS IN JJ NNS CC JJ NN CC NN PRP MD VB DT JJ\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join([tag[1] for tag in tags]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of tags : 199\n"
     ]
    }
   ],
   "source": [
    "# Now let us have a look at the total no of tags. We store all the tags in a variable named pos_tags\n",
    "pos_tags = [tag[1] for tag in tags]\n",
    "#Now we will print the length\n",
    "print(\"No. of tags : \" + str(len(pos_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRP$ VBG VBD VBN VBP WDT JJ VBZ DT NN POS TO PRP RB NNS NNP VB CC CD EX IN MD JJR UH\n",
      "\n",
      "No of unique POSes : 24\n"
     ]
    }
   ],
   "source": [
    "#if you have noticed in entry no. 16 that a lot of tags are repeating. We would like to get all the unique tags from them.\n",
    "#We can simply use he set() data structure to do so which will remove the duplicates.\n",
    "unique_poses = set(pos_tags)\n",
    "print(\" \".join([ i for i in unique_poses ]))\n",
    "print(\"\\nNo of unique POS's : \" + str(len(unique_poses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now you can see that there are only 24 POS tags which have been used and the rest are just repetition. Using TextBlob we can even print all the noun phrases in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['hello', u'animesh shaw', 'computer', 'graduation', 'love', 'computer', u'science subjects', u'computational linguistics', 'love', 'piece', u'naruto shippunden', 'animes', u\"'s goal\", u\"aim 's\", u'own dreams', u'eternal bond', 'naruto', 'piece', u'hard work', u'great heights', u'hard work', u'key goals', u'% people', u'non-prodigy people', u'massive impact', u'great goals', u'constant dedication'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print all the noun phrases\n",
    "tblob.noun_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get all the words as a WordList as well, by using the *words* property as follows which returns a list of all words as a class of WordList. WordList is a list-like collection of words. Its no different from Python lists but with additional methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['Hello', 'My', 'name', 'is', 'Animesh', 'Shaw', 'and', 'I', 'am', 'an', 'undergraduate', 'and', 'studying', 'Computer', 'Science', 'upcoming', 'Graduation', 'in', '2015', 'I', 'Love', 'programming', 'and', 'Computer', 'science', 'subjects', 'of', 'topics', 'My', 'field', 'of', 'interest', 'include', 'Computational', 'Linguistics', 'I', 'Love', 'watching', 'anime', 'specially', 'One', 'Piece', 'and', 'Naruto', 'Shippunden', 'Animes', 'specially', 'those', 'two', 'shows', 'an', 'extravagant', 'amount', 'of', 'dedication', 'passion', 'love', 'and', 'amibition', 'towards', 'achieving', 'one', \"'s\", 'goal', 'and', 'aim', \"'s\", 'in', 'life', 'These', 'have', 'always', 'inpired', 'me', 'a', 'lot', 'Giving', 'up', 'on', 'your', 'own', 'dreams', 'to', 'fulfill', 'others', 'and', 'the', 'same', 'feeling', 'that', 'other', 'carry', 'along', 'with', 'friends', 'or', 'something', 'which', 'I', 'call', 'as', 'an', 'eternal', 'bond', 'I', 'recommend', 'everyone', 'to', 'watch', 'Naruto', 'and', 'One', 'Piece', 'I', 'have', 'learnt', 'a', 'lot', 'from', 'there', 'People', 'are', 'not', 'always', 'born', 'intelligent', 'or', 'powerfull', 'but', 'with', 'hard', 'work', 'great', 'heights', 'can', 'be', 'achieved', 'in', 'life', 'Yes', 'its', 'true', 'dedication', 'and', 'hard', 'work', 'are', 'the', 'key', 'goals', 'to', 'success', 'less', 'than', '1', 'people', 'are', 'born', 'with', 'the', 'blessing', 'of', 'being', 'a', 'prodigy', 'The', 'world', 'was', 'shaken', 'mostly', 'by', 'those', 'non-prodigy', 'people', 'which', 'have', 'had', 'a', 'massive', 'impact', 'in', 'every', 'individuals', 'lives', 'With', 'great', 'goals', 'and', 'constant', 'dedication', 'and', 'passion', 'you', 'can', 'achieve', 'the', 'unachievable'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tblob.words #returns the data as word tokenized form in a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See its so easy. Noun Phrases gives us a lot of important and relevant information which can be further used to analyse the meaning.\n",
    "When we use the **tblob.noun_phrases** it returns the noun_phrases as an WordList which a class used to store words and manipulate them \n",
    "or operate with different functions etc. etc.\n",
    "Lets do something more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Detection and Translation\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that you want to detect the language used in the text above, TextBlob provides a detect_language() method to detect language used.\n",
    "The methods uses the Google Langauge Translate API for the purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'en'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tblob.detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets try some more and in different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'fr'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Okay lets try some more.\n",
    "TextBlob(\"Bonjour\").detect_language()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'it'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Another one\n",
    "TextBlob(\"Ciao\").detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last two **\"fr\"** stands for *french* and **\"it\"** stands for *italian*. Now lets move on.\n",
    "Now that you know that TextBlob can detect language. You might have a question whether it can even do the translation or not. As a matter of fact it can\n",
    "Lets take a simple example we will Convert \"Thanks\" in english to Japanese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"感謝\")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"Thanks\").translate(to=\"ja\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see we got the translated text in Japanese. \n",
    "\n",
    "#### Lets try another example with a bigger sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Bonjour , Mon nom est Animesh P. Shaw . Je vais devenir le roi de programmation\")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"Hello, My name is Animesh P Shaw. I will become the Programming King\").translate(to=\"fr\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be having a doubt whether these returned values are true or not. Well you can always Google you know. \n",
    "Lets see if the last french translated sentence is detected as french or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'fr'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"Bonjour , Mon nom est Animesh P. Shaw . Je vais devenir le roi de programmation\").detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta da! The langauge of the above sentence has been detected as french since **fr** is the french langauge code.\n",
    "## Raw Text Handling\n",
    "\n",
    "Let's explore more and see what we have got. We will print the complete text as raw which means all the escape characters like *\\r* or *\\n* or *\\t* will also be printed. There is an builtin property for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHello, My name is Animesh Shaw and I am an undergraduate and studying Computer Science (upcoming Graduation in 2015). I Love programming and Computer science subjects of topics. \\nMy field of interest include Computational Linguistics. I Love watching anime specially One Piece and Naruto Shippunden. Animes specially those two shows\\nan extravagant amount of dedication, passion love, and amibition towards achieving one\\'s goal and aim\\'s in life. These have always inpired me a lot.\\nGiving up on your own dreams to fulfill others and the same feeling that other carry along with friends or something which I call as an eternal bond.\\nI recommend everyone to watch Naruto and One Piece. I have learnt a lot from there. \"People are not always born intelligent or powerfull but with hard work\\ngreat heights can be achieved in life.\" Yes its true, dedication and hard work are the key goals to success less than 1% people are born with the blessing\\nof being a prodigy. The world was shaken mostly by those non-prodigy people which have had a massive impact in every individuals lives. \\nWith great goals and constant dedication and passion you can achieve \\nthe unachievable.\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tblob.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*raw_sentances* is another property which returns a list of raw sentences which means all the escape characters like \\r or \\n or \\t will also be printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nHello, My name is Animesh Shaw and I am an undergraduate and studying Computer Science (upcoming Graduation in 2015).',\n",
       " 'I Love programming and Computer science subjects of topics.',\n",
       " 'My field of interest include Computational Linguistics.',\n",
       " 'I Love watching anime specially One Piece and Naruto Shippunden.',\n",
       " \"Animes specially those two shows\\nan extravagant amount of dedication, passion love, and amibition towards achieving one's goal and aim's in life.\",\n",
       " 'These have always inpired me a lot.',\n",
       " 'Giving up on your own dreams to fulfill others and the same feeling that other carry along with friends or something which I call as an eternal bond.',\n",
       " 'I recommend everyone to watch Naruto and One Piece.',\n",
       " 'I have learnt a lot from there.',\n",
       " '\"People are not always born intelligent or powerfull but with hard work\\ngreat heights can be achieved in life.\"',\n",
       " 'Yes its true, dedication and hard work are the key goals to success less than 1% people are born with the blessing\\nof being a prodigy.',\n",
       " 'The world was shaken mostly by those non-prodigy people which have had a massive impact in every individuals lives.',\n",
       " 'With great goals and constant dedication and passion you can achieve \\nthe unachievable.\\n']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tblob.raw_sentences #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at another property which is sentences. Now this is different from raw_sentences. The former will return a list of all the sentences of class Sentence().\n",
    "We will have a look at it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"\n",
       "Hello, My name is Animesh Shaw and I am an undergraduate and studying Computer Science (upcoming Graduation in 2015).\"),\n",
       " Sentence(\"I Love programming and Computer science subjects of topics.\"),\n",
       " Sentence(\"My field of interest include Computational Linguistics.\"),\n",
       " Sentence(\"I Love watching anime specially One Piece and Naruto Shippunden.\"),\n",
       " Sentence(\"Animes specially those two shows\n",
       "an extravagant amount of dedication, passion love, and amibition towards achieving one's goal and aim's in life.\"),\n",
       " Sentence(\"These have always inpired me a lot.\"),\n",
       " Sentence(\"Giving up on your own dreams to fulfill others and the same feeling that other carry along with friends or something which I call as an eternal bond.\"),\n",
       " Sentence(\"I recommend everyone to watch Naruto and One Piece.\"),\n",
       " Sentence(\"I have learnt a lot from there.\"),\n",
       " Sentence(\"\"People are not always born intelligent or powerfull but with hard work\n",
       "great heights can be achieved in life.\"\"),\n",
       " Sentence(\"Yes its true, dedication and hard work are the key goals to success less than 1% people are born with the blessing\n",
       "of being a prodigy.\"),\n",
       " Sentence(\"The world was shaken mostly by those non-prodigy people which have had a massive impact in every individuals lives.\"),\n",
       " Sentence(\"With great goals and constant dedication and passion you can achieve \n",
       "the unachievable.\n",
       "\")]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tblob.sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Sentiment Analysis with TextBlob\n",
    "___\n",
    "\n",
    "TextBlob is specially helpful for Sentiment Analysis with all the built in methods and properties which you can modify by configuring and extend with different taggers or Analyzers. \n",
    "\n",
    "### What is Senitiment Analysis ?\n",
    "\n",
    "**Sentiment analysis** (also known as opinion mining) refers to the use of natural language processing, text analysis and computational linguistics to identify and extract subjective information in source materials. With TextBlob we can see both the Polarity and Subjectivity of the information in a sentence or data.\n",
    "\n",
    "Now lets do something interesting and important. Note the following produces important results. \n",
    "We will now see how to measure the polarity of a sentence. Now what is polarity.\n",
    "Polarity is a measure which gives a numerical value depending on which we can understand whether a sentence is postive or negetive. Its more like someone says bad about you feel sad and it means negetive and when someone praises you, you feel joy which is positive polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.5\n",
      "0.0\n",
      "0.428571428571\n",
      "0.428571428571\n",
      "0.0\n",
      "0.158333333333\n",
      "0.0\n",
      "0.0\n",
      "0.436111111111\n",
      "0.0383333333333\n",
      "0.25\n",
      "0.4\n"
     ]
    }
   ],
   "source": [
    "for sent in tblob.sentences:\n",
    "    print(sent.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A value of 0.0 indicates neutral, 0.5 indicates positive. Note that the word \"Love\" indicates postiveness. Values which fall in between 0.4 and 0.5 are almost undecidatble or\n",
    "more or less positve. Let's consider the second last value 0.25, it is low because of the words **shaken** or **massive impact** which infuses a negetive sense.\n",
    "\n",
    "Now lets display both the polarity and subjectivity. The sentiment property returns a namedtuple of the form Sentiment(polarity, subjectivity). The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentiment(polarity=0.5, subjectivity=0.6)\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentiment(polarity=0.4285714285714286, subjectivity=0.5857142857142856)\n",
      "Sentiment(polarity=0.4285714285714286, subjectivity=0.5857142857142856)\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentiment(polarity=0.15833333333333333, subjectivity=0.5)\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Sentiment(polarity=0.4361111111111111, subjectivity=0.7305555555555555)\n",
      "Sentiment(polarity=0.03833333333333332, subjectivity=0.45166666666666666)\n",
      "Sentiment(polarity=0.25, subjectivity=0.75)\n",
      "Sentiment(polarity=0.4, subjectivity=0.5416666666666666)\n"
     ]
    }
   ],
   "source": [
    "for sent in tblob.sentences:\n",
    "    print(sent.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### Dumping Data Properties as JSON\n",
    "\n",
    "Suppose that you want to get all the properties together as one in some format which is efficient and easy to parse. To solve such cases TextBlob provides a way to dump all the properties as a JSON file. For this example we will create a text blob instance with a smaller sentence \"Nico Robin is the most sexy anime character I have ever encountered.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"polarity\": 0.5, \"stripped\": \"nico robin is the most sexy anime character i have ever encountered\", \"noun_phrases\": [\"nico robin\", \"sexy anime character\"], \"raw\": \"Nico Robin is the most sexy anime character I have ever encountered.\", \"subjectivity\": 0.75, \"end_index\": 68, \"start_index\": 0}]\n"
     ]
    }
   ],
   "source": [
    "blob = TextBlob(\"Nico Robin is the most sexy anime character I have ever encountered.\")\n",
    "print(blob.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to represent the JSON data in a serialized manner then you can do this in the following manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'end_index': 68,\n",
       "  u'noun_phrases': WordList([u'nico robin', u'sexy anime character']),\n",
       "  u'polarity': 0.5,\n",
       "  u'raw': 'Nico Robin is the most sexy anime character I have ever encountered.',\n",
       "  u'start_index': 0,\n",
       "  u'stripped': 'nico robin is the most sexy anime character i have ever encountered',\n",
       "  u'subjectivity': 0.75}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.serialized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets test something nice. Suppose that we add the following \" and beautiful lady \" after \"sexy\" in **In [39]** then what changes do you expect to happen. As you know that beautiful is a postive word and so what it does is it increases the polarity value. This technique can be used in different ways in research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"polarity\": 0.6166666666666667, \"stripped\": \"nico robin is the most sexy and beautiful lady anime character i have ever encountered\", \"noun_phrases\": [\"nico robin\", \"beautiful lady anime character\"], \"raw\": \"Nico Robin is the most sexy and beautiful lady anime character I have ever encountered.\", \"subjectivity\": 0.8333333333333334, \"end_index\": 87, \"start_index\": 0}]\n"
     ]
    }
   ],
   "source": [
    "blob = TextBlob(\"Nico Robin is the most sexy and beautiful lady anime character I have ever encountered.\")\n",
    "print(blob.json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Summary\n",
    "\n",
    "So thats all we have came to the end of our first encounter with TextBlob. As you can see I have explained the stuffs in a very detailed manner. This is definatly in more depth than what has been covered in the official tutorials. Stay tuned for more and I will continue this series and explain almost everthing in the TextBlob library. Next time we will discuss about Spelling Corrections, N-Grams, Taggers and maybe lemmatization.\n",
    "\n",
    "Thank you for reading and I hope you have had a nice read."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
